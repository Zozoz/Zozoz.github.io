<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>神经网络权重初始化 | Newbie</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="在神经网络模型训练过程中需要对参数 weight 进行更新，在更新前需要进行初始化操作，当模型足够复杂时，一个好的初始化对模型优化至关重要。
为什么不能全部初始化为0？如果所有参数都是0，那么所有神经元的输出都是相同的，在 forward 和 back 时每一层的神经元的值和梯度都是一样的，使模型过于简单。">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络权重初始化">
<meta property="og:url" content="http://zozoz.github.io/2017/03/18/神经网络权重初始化/index.html">
<meta property="og:site_name" content="Newbie">
<meta property="og:description" content="在神经网络模型训练过程中需要对参数 weight 进行更新，在更新前需要进行初始化操作，当模型足够复杂时，一个好的初始化对模型优化至关重要。
为什么不能全部初始化为0？如果所有参数都是0，那么所有神经元的输出都是相同的，在 forward 和 back 时每一层的神经元的值和梯度都是一样的，使模型过于简单。">
<meta property="og:updated_time" content="2017-03-21T11:19:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络权重初始化">
<meta name="twitter:description" content="在神经网络模型训练过程中需要对参数 weight 进行更新，在更新前需要进行初始化操作，当模型足够复杂时，一个好的初始化对模型优化至关重要。
为什么不能全部初始化为0？如果所有参数都是0，那么所有神经元的输出都是相同的，在 forward 和 back 时每一层的神经元的值和梯度都是一样的，使模型过于简单。">
  
    <link rel="alternate" href="/atom.xml" title="Newbie" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Newbie</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜尋"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://zozoz.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-神经网络权重初始化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/18/神经网络权重初始化/" class="article-date">
  <time datetime="2017-03-18T13:09:45.000Z" itemprop="datePublished">2017-03-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      神经网络权重初始化
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在神经网络模型训练过程中需要对参数 weight 进行更新，在更新前需要进行初始化操作，当模型足够复杂时，一个好的初始化对模型优化至关重要。</p>
<h3 id="为什么不能全部初始化为0？"><a href="#为什么不能全部初始化为0？" class="headerlink" title="为什么不能全部初始化为0？"></a>为什么不能全部初始化为0？</h3><p>如果所有参数都是0，那么所有神经元的输出都是相同的，在 forward 和 back 时每一层的神经元的值和梯度都是一样的，使模型过于简单。</p>
<a id="more"></a>
<h3 id="pre-training"><a href="#pre-training" class="headerlink" title="pre-training"></a>pre-training</h3><p>pre-training一般分两个阶段：① pre-training 阶段，对神经网络的每一层取出构造一个 auto-encoder 做训练，使得输出层能够复现输出层，在这个过程中得到初始参数 ② fine-tuning 阶段，将上一步得到的参数作为初始化参数对模型进行整体调整。</p>
<p>感觉现在不太流行这种做法，个人对此不太熟悉。</p>
<h3 id="高斯分布初始化"><a href="#高斯分布初始化" class="headerlink" title="高斯分布初始化"></a>高斯分布初始化</h3><p>直接对参数进行高斯分布初始化，通常均值为0，标准差为0.01。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(node_in, node_out)) * 0.01</span><br></pre></td></tr></table></figure>
<h3 id="Xavier-初始化"><a href="#Xavier-初始化" class="headerlink" title="Xavier 初始化"></a>Xavier 初始化</h3><p>Xavier 初始化方法来自论文[1]，文章假设激活函数为线性且满足0点处导数为1.</p>
<p>如果<br>$$<br>y = w_1x_1 + \cdots + w_nx_n + b<br>$$<br>根据概率统计知识有下面的方差公式：<br>$$<br>Var(w_ix_i) = E[w_i]^2Var(x_i) + Var(w_i)E(x_i)^2 + Var(w_i)Var(x_i)    \ \ \ \ \  \ \ \ \ (1)<br>$$<br>特别的，当我们假设输入和权重都是零均值时（可以通过Batch Normalization），上式可以简化为：<br>$$<br>Var(w_ix_i) = Var(w_i)Var(x_i)<br>$$<br>进一步假设输入x和权重w独立同分布，则有：<br>$$<br>Var(y) = nVar(w_i)Var(x_i)<br>$$<br>于是，为了保证输入输出方差一致，则应该有：<br>$$<br>Var(w_i) = \frac{1}{n}<br>$$<br>考虑前向和后向传播时每一层的方差一致，应该满足上式，最终我们的权重方差应该满足：<br>$$<br>Var(W) = \frac{2}{node_{in}+node_{out}}<br>$$<br>因为[a,b]区间均匀分布的方差为：<br>$$<br>Var = \frac{(b-a)^2}{12}, E = \frac{a+b}{2}<br>$$<br>因此，Xavier 初始化就是下面的均匀分布：<br>$$<br>W \sim U[-\frac{\sqrt{6}}{\sqrt{(node_{in}+node_{out})}}, \frac{\sqrt{6}}{\sqrt{(node_{in}+node_{out})}}]<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.uniform(-np.sqrt(6/(in+out))), np.sqrt(6/(in+out)))&#10;W = tf.Variable(np.random.randn(in, out)) * np.sqrt(2./(in+out))</span><br></pre></td></tr></table></figure>
<h3 id="MSRA-权重初始化"><a href="#MSRA-权重初始化" class="headerlink" title="MSRA 权重初始化"></a>MSRA 权重初始化</h3><p>MASR 权重初始化方法来自论文[2]，因为 Xavier 推导的时候假设激活函数是线性的，显然常用的激活函数 ReLU 并不满足。推导过程与 Xavier 类似，在得到(1)式后，输入的均值不为0，所以：<br>$$<br>Var(w_ix_i) = Var(w_i)E(x_i)^2 + Var(w_i)Var(x_i) = Var(w_i) E({x_i}^2)<br>$$<br>所以有：<br>$$<br>Var(y) = nVar(w_i)E({x_i}^2)<br>$$<br>对于 ReLU 函数，<br>$$<br>x_l = f(y_{l-1}), \ \ \ \ \ \ \  E(x^2_l) = \frac{1}{2} Var(y_{l-1})<br>$$<br>带入上式得，<br>$$<br>Var(y_l) = \frac{1}{2}n_l Var(w_l)Var(y_{l-1})<br>$$<br>为了使得每一层数据的方差保持一致，则权重应该满足：<br>$$<br>\frac{1}{2}n_l Var(w_l) = 1, \forall l<br>$$<br>即<br>$$<br>Var(W) = \frac{2}{n}<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(in, out)) * np.sqrt(4./(in+out))</span><br></pre></td></tr></table></figure>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><p>BN是在进行非线性激活函数前，对输入值做 normalize，来自论文[3]。<br>$$<br>\mu \leftarrow  \frac{1}{m}\sum_{i=1}^m{x_i}<br>$$</p>
<p>$$<br>\sigma ^2 \leftarrow \frac{1}{m}\sum_{i=1}^m{(x_i-\mu)^2}<br>$$</p>
<p>$$<br>x_i \leftarrow \frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}<br>$$</p>
<p>$$<br>y_i \leftarrow \gamma x_i + \beta<br>$$</p>
<p>BN 中所有的操作都是平滑可导的，这使得反向传播可以有效运行并学到相应的参数。需要注意的一点是 BN 在训练和测试的时候有所差别。训练的时候由当前 batch 计算得出，在测试的时候应该使用训练时保存的均值或类似的经过处理的值，而不是由当前 batch 计算。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layer = tf.contrib.layers.batch_norm(layer, center=True, scale=True, is_training=True)</span><br></pre></td></tr></table></figure>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks[C]//Aistats. 2010, 9: 249-256.</p>
<p>[2] He K, Zhang X, Ren S, et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1026-1034.</p>
<p>[3] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</p>
<p>[4] <a href="http://blog.csdn.net/shuzfan/article/details/51338178" target="_blank" rel="external">http://blog.csdn.net/shuzfan/article/details/51338178</a></p>
<p>[5] <a href="http://blog.csdn.net/shuzfan/article/details/51347572" target="_blank" rel="external">http://blog.csdn.net/shuzfan/article/details/51347572</a></p>
<p>[6] <a href="http://www.leiphone.com/news/201703/3qMp45aQtbxTdzmK.html" target="_blank" rel="external">http://www.leiphone.com/news/201703/3qMp45aQtbxTdzmK.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://zozoz.github.io/2017/03/18/神经网络权重初始化/" data-id="cj0nv763600033pjgb1wwsnyb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NN/">NN</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/23/熵/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          熵
        
      </div>
    </a>
  
  
    <a href="/2017/03/08/常用激活函数优劣对比/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">常用激活函数优劣对比</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">標籤</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BurstDetection/">BurstDetection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NN/">NN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Web/">Web</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/运维/">运维</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">標籤雲</h3>
    <div class="widget tagcloud">
      <a href="/tags/BurstDetection/" style="font-size: 10px;">BurstDetection</a> <a href="/tags/ML/" style="font-size: 15px;">ML</a> <a href="/tags/NN/" style="font-size: 20px;">NN</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a> <a href="/tags/运维/" style="font-size: 10px;">运维</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">彙整</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/03/23/熵/">熵</a>
          </li>
        
          <li>
            <a href="/2017/03/18/神经网络权重初始化/">神经网络权重初始化</a>
          </li>
        
          <li>
            <a href="/2017/03/08/常用激活函数优劣对比/">常用激活函数优劣对比</a>
          </li>
        
          <li>
            <a href="/2016/02/26/如何衡量事件的相似性/">如何衡量事件的相似性</a>
          </li>
        
          <li>
            <a href="/2015/12/16/Kleinberg/">Kleinberg</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 ZOZOZ<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>